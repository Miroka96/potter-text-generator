{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version= 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TF version=',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method for generating text\n",
    "def generate_text(model, length, ix_to_char):\n",
    "    vocab_size = len(ix_to_char)\n",
    "\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "\n",
    "        # magic\n",
    "        ix = np.argmax(model.predict(X[:, :i + 1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "\n",
    "    gen_txt = ''.join(y_char)\n",
    "    try:\n",
    "        print(gen_txt)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return gen_txt\n",
    "\n",
    "\n",
    "# method for preparing the training data\n",
    "def load_data(data_dir, seq_length):\n",
    "    with open(data_dir, 'rb') as bin_data:\n",
    "        data = bin_data.read().decode('utf8', 'ignore')\n",
    "    # data = open(data_dir, 'r').read()\n",
    "    chars = list(set(data))\n",
    "    VOCAB_SIZE = len(chars)\n",
    "\n",
    "    print('Data length: {} characters'.format(len(data)))\n",
    "    print('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
    "\n",
    "    ix_to_char = {ix: char for ix, char in enumerate(chars)}\n",
    "    char_to_ix = {char: ix for ix, char in enumerate(chars)}\n",
    "\n",
    "    X = np.zeros((int(len(data) / seq_length), seq_length, VOCAB_SIZE))\n",
    "    y = np.zeros((int(len(data) / seq_length), seq_length, VOCAB_SIZE))\n",
    "    for i in range(0, int(len(data) / seq_length)):\n",
    "        X_sequence = data[i * seq_length:(i + 1) * seq_length]\n",
    "        X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        for j in range(seq_length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "\n",
    "        y_sequence = data[i * seq_length + 1:(i + 1) * seq_length + 1]\n",
    "        y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        for j in range(seq_length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence\n",
    "    return X, y, ix_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA = 'potter-nlp/final_data/allBooks.txt'\n",
    "BATCH_SIZE = 50\n",
    "LAYER_NUM = 2\n",
    "HIDDEN_DIM = 500\n",
    "SEQ_LENGTH = 50\n",
    "WEIGHTS = ''           # filename of a trained model\n",
    "GENERATE_LENGTH = 500\n",
    "NB_EPOCH = 20\n",
    "MODE = 'train'\n",
    "EPI = 10        # epochs_per_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading probably does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WEIGHTS='tensorflow/checkpoint_layer_2_hidden_500_epoch_390.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WEIGHTS='cntk/checkpoint_layer_2_hidden_500_epoch_460.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backup = MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 6164627 characters\n",
      "Vocabulary size: 57 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/keras/backend/cntk_backend.py:1116: UserWarning: Warning: CNTK backend does not support collapse of batch axis with inferred dimension. The reshape did not take place.\n",
      "  'Warning: CNTK backend does not support '\n",
      "/root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input4\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights for epoch 460\n",
      "sample text (generated):\n",
      "’ba0)b6a9tab1'ba)tt,',)’b6ap:jb1'ba0,6bf),8]b:9’)bf”b817b:tdbj1aa)tb'p□□bf)9\\b'1tb,ad bgv98‘ab,ab79,ab1pa5 b60)b69,’b98’b7,a0b9b□)9jb1'b01tt,f□)b09,t”bft178b1jj16,a)b098’b98’b’178b9b□,aa□)b6:,□)b98’ba1b09tt”‘6b)81t:1p6b)”)6dbg709ab’,’b”1pb69”y bt18b96\\)’b96b0)tb)”)6b'□960)’b96ba01p]0ba0)”b7)t)b6pjj16)’ba1bf)bv1:'1ta9f□”b98’b017ba0)b8)7b6at)8]a0b')□□b6,□)8aba0)b]t)9ab09□□b'1□□17)’b0,:ba0t1p]0ba0)b9,tb98’b)s9:,8,8]ba0)b7,8’176b1'ba0)b□,ft9t”b□11\\,8]b9ab09tt”b'1tb9b:1:)8ab1tba71b9jj9t)8a□”b0)b09’b81\n",
      "\n",
      "\n",
      "Epoch: 460\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input1087\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123292/123292 [==============================] - 102s 829us/step - loss: 3.3519\n",
      "Epoch 2/10\n",
      "123292/123292 [==============================] - 101s 815us/step - loss: 2.2994\n",
      "Epoch 3/10\n",
      "123292/123292 [==============================] - 101s 820us/step - loss: 2.0431\n",
      "Epoch 4/10\n",
      "123292/123292 [==============================] - 101s 821us/step - loss: 2.0003\n",
      "Epoch 5/10\n",
      "123292/123292 [==============================] - 103s 832us/step - loss: 1.9718\n",
      "Epoch 6/10\n",
      "123292/123292 [==============================] - 103s 832us/step - loss: 1.9510\n",
      "Epoch 7/10\n",
      "123292/123292 [==============================] - 102s 827us/step - loss: 1.9347\n",
      "Epoch 8/10\n",
      "123292/123292 [==============================] - 101s 822us/step - loss: 1.9213\n",
      "Epoch 9/10\n",
      "123292/123292 [==============================] - 106s 859us/step - loss: 1.9095\n",
      "Epoch 10/10\n",
      "123292/123292 [==============================] - 104s 843us/step - loss: 1.8995\n",
      "ut of time to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to take to\n",
      "\n",
      "\n",
      "Epoch: 470\n",
      "\n",
      "Epoch 1/10\n",
      "123292/123292 [==============================] - 101s 819us/step - loss: 1.8903\n",
      "Epoch 2/10\n",
      "123292/123292 [==============================] - 101s 820us/step - loss: 1.8822\n",
      "Epoch 3/10\n",
      "123292/123292 [==============================] - 102s 830us/step - loss: 1.8744\n",
      "Epoch 4/10\n",
      "123292/123292 [==============================] - 105s 850us/step - loss: 1.8677\n",
      "Epoch 5/10\n",
      "123292/123292 [==============================] - 104s 843us/step - loss: 1.8613\n",
      "Epoch 6/10\n",
      "123292/123292 [==============================] - 105s 853us/step - loss: 1.8552\n",
      "Epoch 7/10\n",
      "123292/123292 [==============================] - 106s 857us/step - loss: 1.8493\n",
      "Epoch 8/10\n",
      "123292/123292 [==============================] - 106s 857us/step - loss: 1.8435\n",
      "Epoch 9/10\n",
      "123292/123292 [==============================] - 106s 857us/step - loss: 1.8386\n",
      "Epoch 10/10\n",
      "123292/123292 [==============================] - 106s 857us/step - loss: 1.8339\n",
      " to talk to tell to do it and i was a solid wall and saw to take time to take time to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and to talk to you and \n",
      "\n",
      "\n",
      "Epoch: 480\n",
      "\n",
      "Epoch 1/10\n",
      "123292/123292 [==============================] - 106s 858us/step - loss: 1.8298\n",
      "Epoch 2/10\n",
      "123292/123292 [==============================] - 106s 859us/step - loss: 1.8255\n",
      "Epoch 3/10\n",
      "123292/123292 [==============================] - 102s 830us/step - loss: 1.8213\n",
      "Epoch 4/10\n",
      "123292/123292 [==============================] - 106s 860us/step - loss: 1.8180\n",
      "Epoch 5/10\n",
      "123292/123292 [==============================] - 106s 856us/step - loss: 1.8142\n",
      "Epoch 6/10\n",
      "123292/123292 [==============================] - 107s 867us/step - loss: 1.8128\n",
      "Epoch 7/10\n",
      "123292/123292 [==============================] - 102s 829us/step - loss: 1.8087\n",
      "Epoch 8/10\n",
      "123292/123292 [==============================] - 102s 827us/step - loss: 1.8054\n",
      "Epoch 9/10\n",
      "123292/123292 [==============================] - 103s 837us/step - loss: 1.8022\n",
      "Epoch 10/10\n",
      "123292/123292 [==============================] - 106s 861us/step - loss: 1.7998\n",
      "‘dumbledore said “i suppose you are all to tell to get it on top of terror and started to start and took off to talk to tell to talk to you to tell you to stop you to take to tell to talk to you to tell you to stop you to take to tell to talk to you to tell you to stop you to take to tell to talk to you to tell you to stop you to take to tell to talk to you to tell you to stop you to take to tell to talk to you to tell you to stop you to take to tell to talk to you to tell you to stop you to take\n",
      "\n",
      "\n",
      "Epoch: 490\n",
      "\n",
      "Epoch 1/10\n",
      "123292/123292 [==============================] - 107s 867us/step - loss: 1.7964\n",
      "Epoch 2/10\n",
      "123292/123292 [==============================] - 106s 862us/step - loss: 1.7944\n",
      "Epoch 3/10\n",
      "123292/123292 [==============================] - 103s 838us/step - loss: 1.7922\n",
      "Epoch 4/10\n",
      "123292/123292 [==============================] - 104s 844us/step - loss: 1.7923\n",
      "Epoch 5/10\n",
      "123292/123292 [==============================] - 103s 834us/step - loss: 1.7880\n",
      "Epoch 6/10\n",
      "123292/123292 [==============================] - 103s 839us/step - loss: 1.7859\n",
      "Epoch 7/10\n",
      "123292/123292 [==============================] - 110s 893us/step - loss: 1.7839\n",
      "Epoch 8/10\n",
      "123292/123292 [==============================] - 110s 890us/step - loss: 1.7823\n",
      "Epoch 9/10\n",
      "123292/123292 [==============================] - 104s 841us/step - loss: 1.7844\n",
      "Epoch 10/10\n",
      "123292/123292 [==============================] - 101s 821us/step - loss: 1.7794\n",
      "was still standing on tiptoe to ten peter witness to see witcccas to see to take it and to a strange silver stag burned sligger of tea and see to tell to talk to you and tell to know we’re not going to be a moment to see to tell to talk to you and tell to know we’re not going to be a moment to see to tell to talk to you and tell to know we’re not going to be a moment to see to tell to talk to you and tell to know we’re not going to be a moment to see to tell to talk to you and tell to know we’re \n",
      "\n",
      "\n",
      "Epoch: 500\n",
      "\n",
      "Epoch 1/10\n",
      "123292/123292 [==============================] - 102s 825us/step - loss: 1.7768\n",
      "Epoch 2/10\n",
      "123292/123292 [==============================] - 100s 815us/step - loss: 1.7753\n",
      "Epoch 3/10\n",
      "123292/123292 [==============================] - 99s 807us/step - loss: 1.7752\n",
      "Epoch 4/10\n",
      "102750/123292 [========================>.....] - ETA: 16s - loss: 1.7743"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function Trainer_train_minibatch> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function Variable___hash__> returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ae9ec36e73e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nEpoch: {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# sometimes an OSError occurs, especially on Windows 10 Systems...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/keras/backend/cntk_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             result = self.trainer.train_minibatch(\n\u001b[0;32m-> 1969\u001b[0;31m                 input_dict, self.trainer_output)\n\u001b[0m\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/train/trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n\u001b[0;32m--> 171\u001b[0;31m                     output_map, device)\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3027\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function Trainer_train_minibatch> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "# Creating training data\n",
    "X, y, ix_to_char = load_data(DATA, SEQ_LENGTH)\n",
    "VOCAB_SIZE = len(ix_to_char)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # Creating and compiling the Network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "    for i in range(LAYER_NUM - 1):\n",
    "        model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL = create_model()\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    if not WEIGHTS == '':\n",
    "        # Loading the trained weights\n",
    "        MODEL.load_weights(WEIGHTS)\n",
    "        epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "        print('Loaded weights for epoch {}'.format(epoch))\n",
    "    else:\n",
    "        epoch = 0\n",
    "    return epoch\n",
    "\n",
    "\n",
    "actual_epoch = initialize_model()\n",
    "\n",
    "print('sample text (generated):')\n",
    "# Generate some sample before training to know how bad it is!\n",
    "generate_text(MODEL, GENERATE_LENGTH, ix_to_char)\n",
    "\n",
    "if MODE == 'train' or WEIGHTS == '':\n",
    "    while True:\n",
    "        print('\\n\\nEpoch: {}\\n'.format(actual_epoch))\n",
    "        try:\n",
    "            MODEL.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=EPI)\n",
    "        except OSError:\n",
    "            # sometimes an OSError occurs, especially on Windows 10 Systems...\n",
    "            print('Encountered an Error while training this epoch')\n",
    "            continue\n",
    "\n",
    "        actual_epoch += EPI\n",
    "        txt = generate_text(MODEL, GENERATE_LENGTH, ix_to_char)\n",
    "        with open('generated_' + str(actual_epoch), 'wb') as out:\n",
    "            out.write(txt.encode(errors='ignore'))\n",
    "        MODEL.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, actual_epoch))\n",
    "\n",
    "# performing generation only\n",
    "elif MODE == 'test':\n",
    "    if WEIGHTS == '':\n",
    "        print('Specify saved Weights!')\n",
    "        exit(1)\n",
    "\n",
    "    generate_text(MODEL, GENERATE_LENGTH, ix_to_char)\n",
    "    print('\\n\\n')\n",
    "\n",
    "else:\n",
    "    print('\\n\\nNothing to do!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "for file in $(ls generated* | sort -V); do echo $file; echo; cat $file; echo; echo; done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
